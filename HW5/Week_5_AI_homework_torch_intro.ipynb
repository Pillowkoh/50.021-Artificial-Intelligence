{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUbEmuvZJxlI"
      },
      "source": [
        "# PyTorch - homework 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efS07mO7J6AR"
      },
      "source": [
        "Please run the whole notebook with your code and submit the `.ipynb` file that includes your answers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJpzFaX0J6Zz",
        "outputId": "3de0887c-10b5-48db-f1a2-6e5829bc1105",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mHomework by Koh Jun Hao, number: 1004295\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from termcolor import colored\n",
        "\n",
        "student_number = \"1004295\"\n",
        "student_name = \"Koh Jun Hao\"\n",
        "\n",
        "print(colored(\"Homework by \"  + student_name + ', number: ' + student_number,'red'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xDkwBg8LKQ_"
      },
      "source": [
        " ## Question 1 -- matrix multiplication\n",
        "\n",
        "Implement the following mathematical operation on both the CPU and GPU (use Google Colab or another cloud service if you don't have a GPU in your computer). Print:\n",
        "\n",
        "a) which type of GPU card you have and \n",
        "\n",
        "b) show the computation time for both CPU and GPU (using PyTorch). \n",
        "\n",
        "c) How much % fast is the GPU? \n",
        "\n",
        " The operation to implement is the dot product $C = B * A^T$\n",
        "\n",
        " whereby $A$ is a random matrix of size $20,000 \\times 1,000$ and $B$ is a random matrix of size $2,000 \\times 1,000$. In addition to the required information asked above:\n",
        " \n",
        " d) please also print the resulting two $C$ matrices (they should be the same btw). \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BINvhm-PLKak",
        "outputId": "1f2e3b3a-58ee-43a7-e4df-e77acdbb3056",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of GPU card:\n",
            "Tesla T4\n",
            "\n",
            "\n",
            "Time taken for CPU: 2539.18212890625 ms\n",
            "Time taken for GPU: 97.07884979248047 ms\n",
            "\n",
            "\n",
            "GPU is faster by 2515.5873646361742%\n",
            "\n",
            "\n",
            "CPU output is tensor([[262.9389, 262.1331, 258.2029,  ..., 247.3846, 255.4586, 260.5882],\n",
            "        [255.5988, 255.1808, 252.4708,  ..., 242.8665, 251.2313, 255.8275],\n",
            "        [254.6067, 250.8993, 247.8918,  ..., 235.8824, 247.9836, 249.4794],\n",
            "        ...,\n",
            "        [255.2737, 255.6243, 245.6241,  ..., 237.2198, 255.6026, 252.3542],\n",
            "        [249.9145, 253.2180, 248.7569,  ..., 239.2458, 248.9812, 255.1312],\n",
            "        [260.3334, 258.7924, 251.2981,  ..., 243.6195, 261.1603, 259.9942]])\n",
            "\n",
            "GPU output is tensor([[262.9389, 262.1331, 258.2027,  ..., 247.3848, 255.4585, 260.5882],\n",
            "        [255.5988, 255.1809, 252.4709,  ..., 242.8665, 251.2314, 255.8275],\n",
            "        [254.6066, 250.8993, 247.8918,  ..., 235.8824, 247.9835, 249.4793],\n",
            "        ...,\n",
            "        [255.2737, 255.6240, 245.6240,  ..., 237.2198, 255.6026, 252.3545],\n",
            "        [249.9145, 253.2181, 248.7570,  ..., 239.2458, 248.9812, 255.1312],\n",
            "        [260.3333, 258.7923, 251.2982,  ..., 243.6197, 261.1604, 259.9944]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# implement solution here\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "## Part A\n",
        "print(\"Type of GPU card:\")\n",
        "print(torch.cuda.get_device_name())\n",
        "print(\"\\n\")\n",
        "\n",
        "## Part B\n",
        "def dot_product(A_T, B, device):\n",
        "  start = torch.cuda.Event(enable_timing=True)\n",
        "  end = torch.cuda.Event(enable_timing=True)\n",
        "  A_T = A_T.to(device)\n",
        "  B = B.to(device)\n",
        "\n",
        "  start.record()\n",
        "  C = B @ A_T\n",
        "  end.record()\n",
        "  torch.cuda.synchronize()\n",
        "\n",
        "  time_taken = start.elapsed_time(end)\n",
        "  return time_taken, C\n",
        "\n",
        "A = torch.rand(20_000, 1_000)\n",
        "A_T = torch.transpose(A, 0, 1)\n",
        "B = torch.rand(2_000, 1_000)\n",
        "CPU = torch.device(\"cpu\")\n",
        "GPU = torch.device(\"cuda\")\n",
        "\n",
        "CPU_time, CPU_output = dot_product(A_T, B, CPU)\n",
        "GPU_time, GPU_output = dot_product(A_T, B, GPU)\n",
        "\n",
        "print(f\"Time taken for CPU: {CPU_time} ms\")\n",
        "print(f\"Time taken for GPU: {GPU_time} ms\")\n",
        "print(\"\\n\")\n",
        "\n",
        "## Part C\n",
        "speedup = (CPU_time-GPU_time)/GPU_time * 100\n",
        "print(f\"GPU is faster by {speedup}%\")\n",
        "print(\"\\n\")\n",
        "\n",
        "## Part D\n",
        "print(f\"CPU output is {CPU_output}\\n\")\n",
        "print(f\"GPU output is {GPU_output.cpu()}\\n\")\n",
        "# print(f\"Are CPU and GPU outputs the same: {torch.eq(CPU_output, GPU_output.cpu())}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZJXmfT-yU3g"
      },
      "source": [
        "## Question 2 - grad\n",
        "\n",
        "\n",
        "Find the gradient (partial derivatives) of the function $g(w)$ below. \n",
        "\n",
        "Let  $w=[w_1,w_2]^T$\n",
        "\n",
        "Consider  $g(w)=2w_1w_2+w_2cos(w_1)$\n",
        "\n",
        "a) In PyTorch, compute:   $\\nabla g(w)$ \n",
        "\n",
        " and verify that $\\nabla g([\\pi,1])=[2,2\\pi−1]^T$ using the grad function, whereby the first position is the partial for $w_1$ and the second position is the partial for $w_2$. \n",
        "\n",
        "b) You can also write a function to manually calculate these partial derivatives! You can review your differential equations math at [here](https://www.wolframalpha.com/input/?i=derivative+y+cos%28x%29) and implement this as a second function below to verify that it comes to the same solution. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLjz6_LKt4sc",
        "outputId": "f95c6bed-dbb3-4df4-a78a-44f18c79984a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Partial derivative calculated using torch: (tensor(2.), tensor(5.2832))\n",
            "\n",
            "Partial derivative of w1: 2.0\n",
            "Partial derivative of w2: 5.2831854820251465\n"
          ]
        }
      ],
      "source": [
        "# write your solution here\n",
        "\n",
        "## Part A\n",
        "def delta_g(w):\n",
        "  w1 = torch.tensor(w[0], requires_grad=True)\n",
        "  w2 = torch.tensor(w[1], requires_grad=True)\n",
        "  ans = 2*w1*w2 + w2*torch.cos(w1)\n",
        "  ans.backward()\n",
        "  return w1.grad, w2.grad\n",
        "\n",
        "\n",
        "w = [torch.pi, 1.0]\n",
        "print(f\"Partial derivative calculated using torch: {delta_g(w)}\\n\")\n",
        "\n",
        "## Part B\n",
        "def delta_w1(w):\n",
        "  w1 = torch.tensor(w[0], requires_grad=True)\n",
        "  w2 = torch.tensor(w[1], requires_grad=True)\n",
        "  d_w1 = 2*w2 - w2*torch.sin(w1)\n",
        "  return d_w1\n",
        "\n",
        "def delta_w2(w):\n",
        "  w1 = torch.tensor(w[0], requires_grad=True)\n",
        "  w2 = torch.tensor(w[1], requires_grad=True)\n",
        "  d_w2 = 2*w1 + torch.cos(w1)\n",
        "  return d_w2\n",
        "\n",
        "print(f\"Partial derivative of w1: {delta_w1(w)}\")\n",
        "print(f\"Partial derivative of w2: {delta_w2(w)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJwP6ur8LKjD"
      },
      "source": [
        "## Question 3 - dance hit song prediction\n",
        "\n",
        "Implement logistic regression in PyTorch for the following dance hit song prediction training dataset: \n",
        "https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv\n",
        "\n",
        " * Input variables: a number of audio features (most already standardized so don't worry about that)\n",
        " * Target variable: Topclass1030: \n",
        "   * 1 means it was a top 10 hit song; \n",
        "   * 0 means it never went above top 30 position.\n",
        "\n",
        "This dataset is derived from my paper on dance hit song prediction, for full description of features have a look at https://arxiv.org/abs/1905.08076. \n",
        "\n",
        "Print the evolution of the loss every few epochs and train the model until it converges. \n",
        " \n",
        " After training the logistic regression model, calculate the prediction accuracy on the test set: \n",
        " https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ty1e6BrldJm",
        "outputId": "caf7a890-3f22-4ff2-817b-5cfff41832ba",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-16 19:43:23--  https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv\n",
            "Resolving dorax.s3.ap-south-1.amazonaws.com (dorax.s3.ap-south-1.amazonaws.com)... 52.219.64.38\n",
            "Connecting to dorax.s3.ap-south-1.amazonaws.com (dorax.s3.ap-south-1.amazonaws.com)|52.219.64.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147372 (144K) [text/csv]\n",
            "Saving to: ‘herremans_hit_1030training.csv’\n",
            "\n",
            "herremans_hit_1030t 100%[===================>] 143.92K   206KB/s    in 0.7s    \n",
            "\n",
            "2022-06-16 19:43:25 (206 KB/s) - ‘herremans_hit_1030training.csv’ saved [147372/147372]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# load data\n",
        "\n",
        "!wget \"https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv\"\n",
        "\n",
        "csv_file = \"herremans_hit_1030training.csv\"\n",
        "\n",
        "csv = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyRP6bl8t4Wc",
        "outputId": "4596a9f2-0453-4a92-8078-12d2a603a2a2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_size=49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/500, Loss=3.4753434658050537\n",
            "Epoch: 2/500, Loss=3.436222553253174\n",
            "Epoch: 3/500, Loss=3.393145799636841\n",
            "Epoch: 4/500, Loss=3.360952377319336\n",
            "Epoch: 5/500, Loss=3.329829692840576\n",
            "Epoch: 6/500, Loss=3.2979483604431152\n",
            "Epoch: 7/500, Loss=3.2622547149658203\n",
            "Epoch: 8/500, Loss=3.2338969707489014\n",
            "Epoch: 9/500, Loss=3.2153375148773193\n",
            "Epoch: 10/500, Loss=3.198021173477173\n",
            "Epoch: 11/500, Loss=3.1746726036071777\n",
            "Epoch: 12/500, Loss=3.1563801765441895\n",
            "Epoch: 13/500, Loss=3.131575107574463\n",
            "Epoch: 14/500, Loss=3.114480495452881\n",
            "Epoch: 15/500, Loss=3.0878734588623047\n",
            "Epoch: 16/500, Loss=3.087918996810913\n",
            "Epoch: 17/500, Loss=3.072754383087158\n",
            "Epoch: 18/500, Loss=3.0463550090789795\n",
            "Epoch: 19/500, Loss=3.0456206798553467\n",
            "Epoch: 20/500, Loss=3.023674964904785\n",
            "Epoch: 21/500, Loss=3.0187270641326904\n",
            "Epoch: 22/500, Loss=3.0100486278533936\n",
            "Epoch: 23/500, Loss=2.989758253097534\n",
            "Epoch: 24/500, Loss=2.9904823303222656\n",
            "Epoch: 25/500, Loss=2.9774436950683594\n",
            "Epoch: 26/500, Loss=2.9561262130737305\n",
            "Epoch: 27/500, Loss=2.956207036972046\n",
            "Epoch: 28/500, Loss=2.9475295543670654\n",
            "Epoch: 29/500, Loss=2.9390645027160645\n",
            "Epoch: 30/500, Loss=2.9252431392669678\n",
            "Epoch: 31/500, Loss=2.928112030029297\n",
            "Epoch: 32/500, Loss=2.9075140953063965\n",
            "Epoch: 33/500, Loss=2.9136435985565186\n",
            "Epoch: 34/500, Loss=2.9049558639526367\n",
            "Epoch: 35/500, Loss=2.896693229675293\n",
            "Epoch: 36/500, Loss=2.892270565032959\n",
            "Epoch: 37/500, Loss=2.8847994804382324\n",
            "Epoch: 38/500, Loss=2.8788657188415527\n",
            "Epoch: 39/500, Loss=2.872361183166504\n",
            "Epoch: 40/500, Loss=2.8656468391418457\n",
            "Epoch: 41/500, Loss=2.8537721633911133\n",
            "Epoch: 42/500, Loss=2.8545045852661133\n",
            "Epoch: 43/500, Loss=2.8478856086730957\n",
            "Epoch: 44/500, Loss=2.836118221282959\n",
            "Epoch: 45/500, Loss=2.833972454071045\n",
            "Epoch: 46/500, Loss=2.8325228691101074\n",
            "Epoch: 47/500, Loss=2.8181943893432617\n",
            "Epoch: 48/500, Loss=2.802311420440674\n",
            "Epoch: 49/500, Loss=2.818988800048828\n",
            "Epoch: 50/500, Loss=2.8098490238189697\n",
            "Epoch: 51/500, Loss=2.8134560585021973\n",
            "Epoch: 52/500, Loss=2.800969123840332\n",
            "Epoch: 53/500, Loss=2.804544687271118\n",
            "Epoch: 54/500, Loss=2.7885360717773438\n",
            "Epoch: 55/500, Loss=2.7890186309814453\n",
            "Epoch: 56/500, Loss=2.788524627685547\n",
            "Epoch: 57/500, Loss=2.7865309715270996\n",
            "Epoch: 58/500, Loss=2.7791314125061035\n",
            "Epoch: 59/500, Loss=2.781641960144043\n",
            "Epoch: 60/500, Loss=2.7608046531677246\n",
            "Epoch: 61/500, Loss=2.7712485790252686\n",
            "Epoch: 62/500, Loss=2.7682130336761475\n",
            "Epoch: 63/500, Loss=2.7631993293762207\n",
            "Epoch: 64/500, Loss=2.7574610710144043\n",
            "Epoch: 65/500, Loss=2.7488696575164795\n",
            "Epoch: 66/500, Loss=2.757124423980713\n",
            "Epoch: 67/500, Loss=2.738943338394165\n",
            "Epoch: 68/500, Loss=2.7534189224243164\n",
            "Epoch: 69/500, Loss=2.745363712310791\n",
            "Epoch: 70/500, Loss=2.7410531044006348\n",
            "Epoch: 71/500, Loss=2.7385761737823486\n",
            "Epoch: 72/500, Loss=2.734473943710327\n",
            "Epoch: 73/500, Loss=2.7364940643310547\n",
            "Epoch: 74/500, Loss=2.7281155586242676\n",
            "Epoch: 75/500, Loss=2.7296760082244873\n",
            "Epoch: 76/500, Loss=2.7274675369262695\n",
            "Epoch: 77/500, Loss=2.721813201904297\n",
            "Epoch: 78/500, Loss=2.720853805541992\n",
            "Epoch: 79/500, Loss=2.708261489868164\n",
            "Epoch: 80/500, Loss=2.7163376808166504\n",
            "Epoch: 81/500, Loss=2.7042150497436523\n",
            "Epoch: 82/500, Loss=2.70514178276062\n",
            "Epoch: 83/500, Loss=2.7019307613372803\n",
            "Epoch: 84/500, Loss=2.708528995513916\n",
            "Epoch: 85/500, Loss=2.690692186355591\n",
            "Epoch: 86/500, Loss=2.6990602016448975\n",
            "Epoch: 87/500, Loss=2.689757823944092\n",
            "Epoch: 88/500, Loss=2.6880598068237305\n",
            "Epoch: 89/500, Loss=2.670435905456543\n",
            "Epoch: 90/500, Loss=2.692856788635254\n",
            "Epoch: 91/500, Loss=2.687983274459839\n",
            "Epoch: 92/500, Loss=2.672524929046631\n",
            "Epoch: 93/500, Loss=2.687344551086426\n",
            "Epoch: 94/500, Loss=2.682488441467285\n",
            "Epoch: 95/500, Loss=2.677448034286499\n",
            "Epoch: 96/500, Loss=2.681366443634033\n",
            "Epoch: 97/500, Loss=2.6625280380249023\n",
            "Epoch: 98/500, Loss=2.6713664531707764\n",
            "Epoch: 99/500, Loss=2.6773927211761475\n",
            "Epoch: 100/500, Loss=2.6699166297912598\n",
            "Epoch: 101/500, Loss=2.673408269882202\n",
            "Epoch: 102/500, Loss=2.6654529571533203\n",
            "Epoch: 103/500, Loss=2.6603164672851562\n",
            "Epoch: 104/500, Loss=2.648789405822754\n",
            "Epoch: 105/500, Loss=2.6650619506835938\n",
            "Epoch: 106/500, Loss=2.656520128250122\n",
            "Epoch: 107/500, Loss=2.6578550338745117\n",
            "Epoch: 108/500, Loss=2.6607258319854736\n",
            "Epoch: 109/500, Loss=2.637594223022461\n",
            "Epoch: 110/500, Loss=2.6558303833007812\n",
            "Epoch: 111/500, Loss=2.648223638534546\n",
            "Epoch: 112/500, Loss=2.6521425247192383\n",
            "Epoch: 113/500, Loss=2.6329643726348877\n",
            "Epoch: 114/500, Loss=2.6441712379455566\n",
            "Epoch: 115/500, Loss=2.6451241970062256\n",
            "Epoch: 116/500, Loss=2.644535541534424\n",
            "Epoch: 117/500, Loss=2.6412672996520996\n",
            "Epoch: 118/500, Loss=2.640139579772949\n",
            "Epoch: 119/500, Loss=2.6397461891174316\n",
            "Epoch: 120/500, Loss=2.633289337158203\n",
            "Epoch: 121/500, Loss=2.6316723823547363\n",
            "Epoch: 122/500, Loss=2.6358981132507324\n",
            "Epoch: 123/500, Loss=2.6344921588897705\n",
            "Epoch: 124/500, Loss=2.625507116317749\n",
            "Epoch: 125/500, Loss=2.6332459449768066\n",
            "Epoch: 126/500, Loss=2.622368335723877\n",
            "Epoch: 127/500, Loss=2.6322715282440186\n",
            "Epoch: 128/500, Loss=2.6211137771606445\n",
            "Epoch: 129/500, Loss=2.628486394882202\n",
            "Epoch: 130/500, Loss=2.6214523315429688\n",
            "Epoch: 131/500, Loss=2.6229569911956787\n",
            "Epoch: 132/500, Loss=2.626316547393799\n",
            "Epoch: 133/500, Loss=2.612367630004883\n",
            "Epoch: 134/500, Loss=2.623877763748169\n",
            "Epoch: 135/500, Loss=2.6129541397094727\n",
            "Epoch: 136/500, Loss=2.61711049079895\n",
            "Epoch: 137/500, Loss=2.613443374633789\n",
            "Epoch: 138/500, Loss=2.6110129356384277\n",
            "Epoch: 139/500, Loss=2.6134252548217773\n",
            "Epoch: 140/500, Loss=2.6048970222473145\n",
            "Epoch: 141/500, Loss=2.6159260272979736\n",
            "Epoch: 142/500, Loss=2.6113038063049316\n",
            "Epoch: 143/500, Loss=2.611663579940796\n",
            "Epoch: 144/500, Loss=2.6072440147399902\n",
            "Epoch: 145/500, Loss=2.6097373962402344\n",
            "Epoch: 146/500, Loss=2.6075994968414307\n",
            "Epoch: 147/500, Loss=2.609196186065674\n",
            "Epoch: 148/500, Loss=2.5870795249938965\n",
            "Epoch: 149/500, Loss=2.5931642055511475\n",
            "Epoch: 150/500, Loss=2.598879337310791\n",
            "Epoch: 151/500, Loss=2.588009834289551\n",
            "Epoch: 152/500, Loss=2.602937936782837\n",
            "Epoch: 153/500, Loss=2.5960872173309326\n",
            "Epoch: 154/500, Loss=2.5951225757598877\n",
            "Epoch: 155/500, Loss=2.587059736251831\n",
            "Epoch: 156/500, Loss=2.5914103984832764\n",
            "Epoch: 157/500, Loss=2.581137180328369\n",
            "Epoch: 158/500, Loss=2.5993471145629883\n",
            "Epoch: 159/500, Loss=2.594317674636841\n",
            "Epoch: 160/500, Loss=2.5855047702789307\n",
            "Epoch: 161/500, Loss=2.5869762897491455\n",
            "Epoch: 162/500, Loss=2.5915257930755615\n",
            "Epoch: 163/500, Loss=2.5869901180267334\n",
            "Epoch: 164/500, Loss=2.5862207412719727\n",
            "Epoch: 165/500, Loss=2.5801501274108887\n",
            "Epoch: 166/500, Loss=2.5878896713256836\n",
            "Epoch: 167/500, Loss=2.588658332824707\n",
            "Epoch: 168/500, Loss=2.5859076976776123\n",
            "Epoch: 169/500, Loss=2.5831551551818848\n",
            "Epoch: 170/500, Loss=2.581066370010376\n",
            "Epoch: 171/500, Loss=2.5842251777648926\n",
            "Epoch: 172/500, Loss=2.5854716300964355\n",
            "Epoch: 173/500, Loss=2.58183217048645\n",
            "Epoch: 174/500, Loss=2.571227550506592\n",
            "Epoch: 175/500, Loss=2.5776779651641846\n",
            "Epoch: 176/500, Loss=2.5787675380706787\n",
            "Epoch: 177/500, Loss=2.5819945335388184\n",
            "Epoch: 178/500, Loss=2.5705740451812744\n",
            "Epoch: 179/500, Loss=2.5827159881591797\n",
            "Epoch: 180/500, Loss=2.5789763927459717\n",
            "Epoch: 181/500, Loss=2.580167293548584\n",
            "Epoch: 182/500, Loss=2.5771398544311523\n",
            "Epoch: 183/500, Loss=2.572958469390869\n",
            "Epoch: 184/500, Loss=2.5762672424316406\n",
            "Epoch: 185/500, Loss=2.5740602016448975\n",
            "Epoch: 186/500, Loss=2.5582573413848877\n",
            "Epoch: 187/500, Loss=2.5776126384735107\n",
            "Epoch: 188/500, Loss=2.553065538406372\n",
            "Epoch: 189/500, Loss=2.5557518005371094\n",
            "Epoch: 190/500, Loss=2.569514036178589\n",
            "Epoch: 191/500, Loss=2.5716822147369385\n",
            "Epoch: 192/500, Loss=2.572082996368408\n",
            "Epoch: 193/500, Loss=2.568686008453369\n",
            "Epoch: 194/500, Loss=2.5714433193206787\n",
            "Epoch: 195/500, Loss=2.5695555210113525\n",
            "Epoch: 196/500, Loss=2.567164659500122\n",
            "Epoch: 197/500, Loss=2.5645105838775635\n",
            "Epoch: 198/500, Loss=2.559195041656494\n",
            "Epoch: 199/500, Loss=2.5567126274108887\n",
            "Epoch: 200/500, Loss=2.5682382583618164\n",
            "Epoch: 201/500, Loss=2.5482327938079834\n",
            "Epoch: 202/500, Loss=2.5666511058807373\n",
            "Epoch: 203/500, Loss=2.5614726543426514\n",
            "Epoch: 204/500, Loss=2.550382137298584\n",
            "Epoch: 205/500, Loss=2.562936544418335\n",
            "Epoch: 206/500, Loss=2.5587475299835205\n",
            "Epoch: 207/500, Loss=2.561211109161377\n",
            "Epoch: 208/500, Loss=2.558744430541992\n",
            "Epoch: 209/500, Loss=2.547100782394409\n",
            "Epoch: 210/500, Loss=2.5619592666625977\n",
            "Epoch: 211/500, Loss=2.561540126800537\n",
            "Epoch: 212/500, Loss=2.5618205070495605\n",
            "Epoch: 213/500, Loss=2.557307481765747\n",
            "Epoch: 214/500, Loss=2.5598604679107666\n",
            "Epoch: 215/500, Loss=2.54658842086792\n",
            "Epoch: 216/500, Loss=2.558307647705078\n",
            "Epoch: 217/500, Loss=2.536442995071411\n",
            "Epoch: 218/500, Loss=2.5425467491149902\n",
            "Epoch: 219/500, Loss=2.554227590560913\n",
            "Epoch: 220/500, Loss=2.5479354858398438\n",
            "Epoch: 221/500, Loss=2.5467922687530518\n",
            "Epoch: 222/500, Loss=2.5535295009613037\n",
            "Epoch: 223/500, Loss=2.5550742149353027\n",
            "Epoch: 224/500, Loss=2.556758403778076\n",
            "Epoch: 225/500, Loss=2.549146890640259\n",
            "Epoch: 226/500, Loss=2.5534181594848633\n",
            "Epoch: 227/500, Loss=2.5536623001098633\n",
            "Epoch: 228/500, Loss=2.5538840293884277\n",
            "Epoch: 229/500, Loss=2.5537686347961426\n",
            "Epoch: 230/500, Loss=2.550625801086426\n",
            "Epoch: 231/500, Loss=2.5505361557006836\n",
            "Epoch: 232/500, Loss=2.541592836380005\n",
            "Epoch: 233/500, Loss=2.541698694229126\n",
            "Epoch: 234/500, Loss=2.550386428833008\n",
            "Epoch: 235/500, Loss=2.5505211353302\n",
            "Epoch: 236/500, Loss=2.5323872566223145\n",
            "Epoch: 237/500, Loss=2.539083957672119\n",
            "Epoch: 238/500, Loss=2.5462164878845215\n",
            "Epoch: 239/500, Loss=2.5185675621032715\n",
            "Epoch: 240/500, Loss=2.5382490158081055\n",
            "Epoch: 241/500, Loss=2.5455808639526367\n",
            "Epoch: 242/500, Loss=2.5365703105926514\n",
            "Epoch: 243/500, Loss=2.5327794551849365\n",
            "Epoch: 244/500, Loss=2.5466575622558594\n",
            "Epoch: 245/500, Loss=2.529245615005493\n",
            "Epoch: 246/500, Loss=2.5267443656921387\n",
            "Epoch: 247/500, Loss=2.5428006649017334\n",
            "Epoch: 248/500, Loss=2.536031723022461\n",
            "Epoch: 249/500, Loss=2.5398941040039062\n",
            "Epoch: 250/500, Loss=2.533949851989746\n",
            "Epoch: 251/500, Loss=2.5315520763397217\n",
            "Epoch: 252/500, Loss=2.5238516330718994\n",
            "Epoch: 253/500, Loss=2.5294437408447266\n",
            "Epoch: 254/500, Loss=2.5349550247192383\n",
            "Epoch: 255/500, Loss=2.539517402648926\n",
            "Epoch: 256/500, Loss=2.5310511589050293\n",
            "Epoch: 257/500, Loss=2.5395240783691406\n",
            "Epoch: 258/500, Loss=2.5284082889556885\n",
            "Epoch: 259/500, Loss=2.5340576171875\n",
            "Epoch: 260/500, Loss=2.5242576599121094\n",
            "Epoch: 261/500, Loss=2.5176565647125244\n",
            "Epoch: 262/500, Loss=2.537184715270996\n",
            "Epoch: 263/500, Loss=2.540015697479248\n",
            "Epoch: 264/500, Loss=2.535461187362671\n",
            "Epoch: 265/500, Loss=2.529343843460083\n",
            "Epoch: 266/500, Loss=2.5358920097351074\n",
            "Epoch: 267/500, Loss=2.53352689743042\n",
            "Epoch: 268/500, Loss=2.53804349899292\n",
            "Epoch: 269/500, Loss=2.539180278778076\n",
            "Epoch: 270/500, Loss=2.5374462604522705\n",
            "Epoch: 271/500, Loss=2.535076856613159\n",
            "Epoch: 272/500, Loss=2.535898447036743\n",
            "Epoch: 273/500, Loss=2.5302248001098633\n",
            "Epoch: 274/500, Loss=2.5301263332366943\n",
            "Epoch: 275/500, Loss=2.5313377380371094\n",
            "Epoch: 276/500, Loss=2.532449245452881\n",
            "Epoch: 277/500, Loss=2.5155012607574463\n",
            "Epoch: 278/500, Loss=2.534951686859131\n",
            "Epoch: 279/500, Loss=2.5323519706726074\n",
            "Epoch: 280/500, Loss=2.5355570316314697\n",
            "Epoch: 281/500, Loss=2.530574321746826\n",
            "Epoch: 282/500, Loss=2.5189568996429443\n",
            "Epoch: 283/500, Loss=2.526319742202759\n",
            "Epoch: 284/500, Loss=2.5302939414978027\n",
            "Epoch: 285/500, Loss=2.5259323120117188\n",
            "Epoch: 286/500, Loss=2.496859550476074\n",
            "Epoch: 287/500, Loss=2.523850917816162\n",
            "Epoch: 288/500, Loss=2.531785488128662\n",
            "Epoch: 289/500, Loss=2.517411231994629\n",
            "Epoch: 290/500, Loss=2.523808479309082\n",
            "Epoch: 291/500, Loss=2.522933006286621\n",
            "Epoch: 292/500, Loss=2.5149998664855957\n",
            "Epoch: 293/500, Loss=2.527994155883789\n",
            "Epoch: 294/500, Loss=2.52583646774292\n",
            "Epoch: 295/500, Loss=2.5130131244659424\n",
            "Epoch: 296/500, Loss=2.5256361961364746\n",
            "Epoch: 297/500, Loss=2.5232999324798584\n",
            "Epoch: 298/500, Loss=2.517038106918335\n",
            "Epoch: 299/500, Loss=2.527414321899414\n",
            "Epoch: 300/500, Loss=2.5275068283081055\n",
            "Epoch: 301/500, Loss=2.5298709869384766\n",
            "Epoch: 302/500, Loss=2.5268287658691406\n",
            "Epoch: 303/500, Loss=2.52700138092041\n",
            "Epoch: 304/500, Loss=2.5271716117858887\n",
            "Epoch: 305/500, Loss=2.525698184967041\n",
            "Epoch: 306/500, Loss=2.5168306827545166\n",
            "Epoch: 307/500, Loss=2.5257604122161865\n",
            "Epoch: 308/500, Loss=2.525287628173828\n",
            "Epoch: 309/500, Loss=2.5189261436462402\n",
            "Epoch: 310/500, Loss=2.518589496612549\n",
            "Epoch: 311/500, Loss=2.5193350315093994\n",
            "Epoch: 312/500, Loss=2.513904571533203\n",
            "Epoch: 313/500, Loss=2.5111851692199707\n",
            "Epoch: 314/500, Loss=2.5253636837005615\n",
            "Epoch: 315/500, Loss=2.522779941558838\n",
            "Epoch: 316/500, Loss=2.5239598751068115\n",
            "Epoch: 317/500, Loss=2.5243239402770996\n",
            "Epoch: 318/500, Loss=2.5220587253570557\n",
            "Epoch: 319/500, Loss=2.516000986099243\n",
            "Epoch: 320/500, Loss=2.524519920349121\n",
            "Epoch: 321/500, Loss=2.515268564224243\n",
            "Epoch: 322/500, Loss=2.5219290256500244\n",
            "Epoch: 323/500, Loss=2.5205531120300293\n",
            "Epoch: 324/500, Loss=2.514754295349121\n",
            "Epoch: 325/500, Loss=2.5084500312805176\n",
            "Epoch: 326/500, Loss=2.5152297019958496\n",
            "Epoch: 327/500, Loss=2.522859573364258\n",
            "Epoch: 328/500, Loss=2.5197455883026123\n",
            "Epoch: 329/500, Loss=2.5142197608947754\n",
            "Epoch: 330/500, Loss=2.510908603668213\n",
            "Epoch: 331/500, Loss=2.5202527046203613\n",
            "Epoch: 332/500, Loss=2.5172767639160156\n",
            "Epoch: 333/500, Loss=2.490805149078369\n",
            "Epoch: 334/500, Loss=2.505998373031616\n",
            "Epoch: 335/500, Loss=2.5174124240875244\n",
            "Epoch: 336/500, Loss=2.5101377964019775\n",
            "Epoch: 337/500, Loss=2.507319211959839\n",
            "Epoch: 338/500, Loss=2.5077459812164307\n",
            "Epoch: 339/500, Loss=2.5096049308776855\n",
            "Epoch: 340/500, Loss=2.5187811851501465\n",
            "Epoch: 341/500, Loss=2.519117593765259\n",
            "Epoch: 342/500, Loss=2.5146408081054688\n",
            "Epoch: 343/500, Loss=2.510953903198242\n",
            "Epoch: 344/500, Loss=2.519456624984741\n",
            "Epoch: 345/500, Loss=2.51529860496521\n",
            "Epoch: 346/500, Loss=2.5037941932678223\n",
            "Epoch: 347/500, Loss=2.517220973968506\n",
            "Epoch: 348/500, Loss=2.5188729763031006\n",
            "Epoch: 349/500, Loss=2.5092365741729736\n",
            "Epoch: 350/500, Loss=2.510187864303589\n",
            "Epoch: 351/500, Loss=2.5172219276428223\n",
            "Epoch: 352/500, Loss=2.501617431640625\n",
            "Epoch: 353/500, Loss=2.5135953426361084\n",
            "Epoch: 354/500, Loss=2.5140140056610107\n",
            "Epoch: 355/500, Loss=2.514939785003662\n",
            "Epoch: 356/500, Loss=2.498760938644409\n",
            "Epoch: 357/500, Loss=2.510838031768799\n",
            "Epoch: 358/500, Loss=2.5008771419525146\n",
            "Epoch: 359/500, Loss=2.5016965866088867\n",
            "Epoch: 360/500, Loss=2.5026228427886963\n",
            "Epoch: 361/500, Loss=2.505159616470337\n",
            "Epoch: 362/500, Loss=2.513336658477783\n",
            "Epoch: 363/500, Loss=2.5161614418029785\n",
            "Epoch: 364/500, Loss=2.5019359588623047\n",
            "Epoch: 365/500, Loss=2.5131123065948486\n",
            "Epoch: 366/500, Loss=2.5018951892852783\n",
            "Epoch: 367/500, Loss=2.494951009750366\n",
            "Epoch: 368/500, Loss=2.506089210510254\n",
            "Epoch: 369/500, Loss=2.507448434829712\n",
            "Epoch: 370/500, Loss=2.5026912689208984\n",
            "Epoch: 371/500, Loss=2.508054256439209\n",
            "Epoch: 372/500, Loss=2.5141520500183105\n",
            "Epoch: 373/500, Loss=2.505892515182495\n",
            "Epoch: 374/500, Loss=2.5088586807250977\n",
            "Epoch: 375/500, Loss=2.5113353729248047\n",
            "Epoch: 376/500, Loss=2.5053927898406982\n",
            "Epoch: 377/500, Loss=2.5055322647094727\n",
            "Epoch: 378/500, Loss=2.5093495845794678\n",
            "Epoch: 379/500, Loss=2.491694688796997\n",
            "Epoch: 380/500, Loss=2.4862053394317627\n",
            "Epoch: 381/500, Loss=2.5130813121795654\n",
            "Epoch: 382/500, Loss=2.49615740776062\n",
            "Epoch: 383/500, Loss=2.501821279525757\n",
            "Epoch: 384/500, Loss=2.503277540206909\n",
            "Epoch: 385/500, Loss=2.5012481212615967\n",
            "Epoch: 386/500, Loss=2.4972317218780518\n",
            "Epoch: 387/500, Loss=2.511789560317993\n",
            "Epoch: 388/500, Loss=2.510023832321167\n",
            "Epoch: 389/500, Loss=2.5106184482574463\n",
            "Epoch: 390/500, Loss=2.506753921508789\n",
            "Epoch: 391/500, Loss=2.5115389823913574\n",
            "Epoch: 392/500, Loss=2.507721424102783\n",
            "Epoch: 393/500, Loss=2.5073657035827637\n",
            "Epoch: 394/500, Loss=2.5028586387634277\n",
            "Epoch: 395/500, Loss=2.5025951862335205\n",
            "Epoch: 396/500, Loss=2.5021040439605713\n",
            "Epoch: 397/500, Loss=2.486217737197876\n",
            "Epoch: 398/500, Loss=2.50506329536438\n",
            "Epoch: 399/500, Loss=2.4949896335601807\n",
            "Epoch: 400/500, Loss=2.503601551055908\n",
            "Epoch: 401/500, Loss=2.5101187229156494\n",
            "Epoch: 402/500, Loss=2.5085411071777344\n",
            "Epoch: 403/500, Loss=2.485017776489258\n",
            "Epoch: 404/500, Loss=2.502149820327759\n",
            "Epoch: 405/500, Loss=2.503756523132324\n",
            "Epoch: 406/500, Loss=2.49920916557312\n",
            "Epoch: 407/500, Loss=2.493922233581543\n",
            "Epoch: 408/500, Loss=2.507204294204712\n",
            "Epoch: 409/500, Loss=2.5047831535339355\n",
            "Epoch: 410/500, Loss=2.499746084213257\n",
            "Epoch: 411/500, Loss=2.5062146186828613\n",
            "Epoch: 412/500, Loss=2.5056586265563965\n",
            "Epoch: 413/500, Loss=2.5074033737182617\n",
            "Epoch: 414/500, Loss=2.506800651550293\n",
            "Epoch: 415/500, Loss=2.498518943786621\n",
            "Epoch: 416/500, Loss=2.5009138584136963\n",
            "Epoch: 417/500, Loss=2.488332748413086\n",
            "Epoch: 418/500, Loss=2.5061070919036865\n",
            "Epoch: 419/500, Loss=2.4920332431793213\n",
            "Epoch: 420/500, Loss=2.485374927520752\n",
            "Epoch: 421/500, Loss=2.487191915512085\n",
            "Epoch: 422/500, Loss=2.4987266063690186\n",
            "Epoch: 423/500, Loss=2.5011863708496094\n",
            "Epoch: 424/500, Loss=2.505362033843994\n",
            "Epoch: 425/500, Loss=2.5055618286132812\n",
            "Epoch: 426/500, Loss=2.46832275390625\n",
            "Epoch: 427/500, Loss=2.5015323162078857\n",
            "Epoch: 428/500, Loss=2.498737335205078\n",
            "Epoch: 429/500, Loss=2.5020029544830322\n",
            "Epoch: 430/500, Loss=2.4894752502441406\n",
            "Epoch: 431/500, Loss=2.5036613941192627\n",
            "Epoch: 432/500, Loss=2.502880334854126\n",
            "Epoch: 433/500, Loss=2.5047249794006348\n",
            "Epoch: 434/500, Loss=2.490445852279663\n",
            "Epoch: 435/500, Loss=2.5052623748779297\n",
            "Epoch: 436/500, Loss=2.5032424926757812\n",
            "Epoch: 437/500, Loss=2.4925591945648193\n",
            "Epoch: 438/500, Loss=2.493061065673828\n",
            "Epoch: 439/500, Loss=2.5012400150299072\n",
            "Epoch: 440/500, Loss=2.4943408966064453\n",
            "Epoch: 441/500, Loss=2.496981620788574\n",
            "Epoch: 442/500, Loss=2.4956953525543213\n",
            "Epoch: 443/500, Loss=2.5028274059295654\n",
            "Epoch: 444/500, Loss=2.490506649017334\n",
            "Epoch: 445/500, Loss=2.5039725303649902\n",
            "Epoch: 446/500, Loss=2.4764864444732666\n",
            "Epoch: 447/500, Loss=2.5009870529174805\n",
            "Epoch: 448/500, Loss=2.4964582920074463\n",
            "Epoch: 449/500, Loss=2.4908559322357178\n",
            "Epoch: 450/500, Loss=2.5011746883392334\n",
            "Epoch: 451/500, Loss=2.4843056201934814\n",
            "Epoch: 452/500, Loss=2.501875638961792\n",
            "Epoch: 453/500, Loss=2.495952606201172\n",
            "Epoch: 454/500, Loss=2.4985594749450684\n",
            "Epoch: 455/500, Loss=2.4969677925109863\n",
            "Epoch: 456/500, Loss=2.4978740215301514\n",
            "Epoch: 457/500, Loss=2.4995787143707275\n",
            "Epoch: 458/500, Loss=2.489610195159912\n",
            "Epoch: 459/500, Loss=2.4992828369140625\n",
            "Epoch: 460/500, Loss=2.4988534450531006\n",
            "Epoch: 461/500, Loss=2.499642848968506\n",
            "Epoch: 462/500, Loss=2.4957962036132812\n",
            "Epoch: 463/500, Loss=2.502331256866455\n",
            "Epoch: 464/500, Loss=2.4933669567108154\n",
            "Epoch: 465/500, Loss=2.4980430603027344\n",
            "Epoch: 466/500, Loss=2.5012316703796387\n",
            "Epoch: 467/500, Loss=2.4979469776153564\n",
            "Epoch: 468/500, Loss=2.4975290298461914\n",
            "Epoch: 469/500, Loss=2.4996073246002197\n",
            "Epoch: 470/500, Loss=2.4998905658721924\n",
            "Epoch: 471/500, Loss=2.4991114139556885\n",
            "Epoch: 472/500, Loss=2.4916694164276123\n",
            "Epoch: 473/500, Loss=2.4836854934692383\n",
            "Epoch: 474/500, Loss=2.4955034255981445\n",
            "Epoch: 475/500, Loss=2.491170883178711\n",
            "Epoch: 476/500, Loss=2.489692211151123\n",
            "Epoch: 477/500, Loss=2.4973373413085938\n",
            "Epoch: 478/500, Loss=2.496398448944092\n",
            "Epoch: 479/500, Loss=2.498340129852295\n",
            "Epoch: 480/500, Loss=2.4966742992401123\n",
            "Epoch: 481/500, Loss=2.4940714836120605\n",
            "Epoch: 482/500, Loss=2.497431516647339\n",
            "Epoch: 483/500, Loss=2.495051622390747\n",
            "Epoch: 484/500, Loss=2.4987571239471436\n",
            "Epoch: 485/500, Loss=2.49918532371521\n",
            "Epoch: 486/500, Loss=2.4923198223114014\n",
            "Epoch: 487/500, Loss=2.4883203506469727\n",
            "Epoch: 488/500, Loss=2.491365909576416\n",
            "Epoch: 489/500, Loss=2.4952104091644287\n",
            "Epoch: 490/500, Loss=2.4976563453674316\n",
            "Epoch: 491/500, Loss=2.4993703365325928\n",
            "Epoch: 492/500, Loss=2.494351387023926\n",
            "Epoch: 493/500, Loss=2.4964535236358643\n",
            "Epoch: 494/500, Loss=2.4972362518310547\n",
            "Epoch: 495/500, Loss=2.496149778366089\n",
            "Epoch: 496/500, Loss=2.493803024291992\n",
            "Epoch: 497/500, Loss=2.487105131149292\n",
            "Epoch: 498/500, Loss=2.482841968536377\n",
            "Epoch: 499/500, Loss=2.479912757873535\n",
            "Epoch: 500/500, Loss=2.495213031768799\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "\n",
        "csv_file = \"herremans_hit_1030training.csv\"\n",
        "\n",
        "class danceSongData(Dataset):\n",
        "  def __init__(self, csv_file):\n",
        "    self.csv_file = pd.read_csv(csv_file)\n",
        "    self.x = torch.tensor(self.csv_file.iloc[:,:-1].values, dtype=torch.float32)\n",
        "    self.y = torch.tensor(self.csv_file.iloc[:,-1].values, dtype=torch.float32)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.y.shape[0]\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]\n",
        "\n",
        "\n",
        "  def num_feature(self):\n",
        "    return self.x.shape[1]\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "  def __init__(self, model, loss_fn, optimiser, config):\n",
        "    self.model = model\n",
        "    self.loss_fn = loss_fn\n",
        "    self.optimiser = optimiser\n",
        "    self.loss = 0\n",
        "    self.epochs = config[\"EPOCHS\"]\n",
        "    self.batch_size = config[\"BATCH_SIZE\"]\n",
        "  \n",
        "\n",
        "  def train(self, train_dataloader):\n",
        "    for epoch in range(self.epochs):\n",
        "      self._epoch_train(train_dataloader)\n",
        "      print(f\"Epoch: {epoch+1}/{self.epochs}, Loss={self.loss}\")\n",
        "\n",
        "\n",
        "  def _epoch_train(self, dataloader):\n",
        "    self.loss = 0\n",
        "    for __, data in enumerate(dataloader, 0):\n",
        "      x, y = data\n",
        "      self.optimiser.zero_grad()\n",
        "      prediction = self.model(x).flatten()\n",
        "\n",
        "      loss = 0\n",
        "\n",
        "      for i in range(self.batch_size):\n",
        "        loss += self.loss_fn(prediction[i], y[i])\n",
        "\n",
        "      loss /= self.batch_size\n",
        "\n",
        "      loss.backward()\n",
        "      self.optimiser.step()\n",
        "\n",
        "      self.loss += loss\n",
        "\n",
        "\n",
        "# define logistic regression model\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  # input_size: Dimensionality of input feature vector.\n",
        "  # num_classes: The number of classes in the classification problem.\n",
        "  def __init__(self, input_size, num_classes):\n",
        "    # Always call the superclass (nn.Module) constructor first!\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    # Set up the linear transform\n",
        "    self.linear = nn.Linear(input_size, num_classes)\n",
        "    # I do not yet include the sigmoid activation after the linear \n",
        "    # layer because our loss function will include this as you will see later\n",
        "\n",
        "  # Forward's sole argument is the input.\n",
        "  # input is of shape (batch_size, input_size)\n",
        "  def forward(self, x):\n",
        "    # Apply the linear transform.\n",
        "    # out is of shape (batch_size, num_classes). \n",
        "    out = self.linear(x)\n",
        "    out = torch.sigmoid(out)\n",
        "    # Softmax the out tensor to get a log-probability distribution\n",
        "    # over classes for each example.\n",
        "    return out\n",
        "\n",
        "# train model\n",
        "config = {\n",
        "    \"DEVICE\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"BATCH_SIZE\": 64,\n",
        "    \"EPOCHS\": 500,\n",
        "    \"LR\": 1e-3,\n",
        "}\n",
        "\n",
        "train_dataset = danceSongData(csv_file)\n",
        "\n",
        "input_size = train_dataset.num_feature()\n",
        "print(f\"input_size={input_size}\")\n",
        "num_classes = 1\n",
        "model = LogisticRegression(input_size, num_classes)\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=config[\"LR\"])\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=config[\"BATCH_SIZE\"],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=4\n",
        "  )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    loss_fn=loss_fn,\n",
        "    optimiser=optimiser,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "trainer.train(train_dataloader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw4yfGoGuChe"
      },
      "source": [
        "Run the below code to test the accuracy of your model on the training set: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsJBgwBn6RO-",
        "outputId": "75e1a195-2cce-4fdd-cc0a-0353766ed662",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-16 19:45:42--  https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv\n",
            "Resolving dorax.s3.ap-south-1.amazonaws.com (dorax.s3.ap-south-1.amazonaws.com)... 52.219.62.103\n",
            "Connecting to dorax.s3.ap-south-1.amazonaws.com (dorax.s3.ap-south-1.amazonaws.com)|52.219.62.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36712 (36K) [text/csv]\n",
            "Saving to: ‘herremans_hit_1030test.csv’\n",
            "\n",
            "herremans_hit_1030t 100%[===================>]  35.85K   154KB/s    in 0.2s    \n",
            "\n",
            "2022-06-16 19:45:43 (154 KB/s) - ‘herremans_hit_1030test.csv’ saved [36712/36712]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L88WmKtMt5gH",
        "outputId": "8cb4abdc-1544-49ec-e71a-fd053657f895",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positives: 43, True Negatives: 17\n",
            "False Positives: 12, False Negatives: 7\n",
            "Class specific accuracy of correctly predicting a hit song is 0.86\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd \n",
        "\n",
        "test = pd.read_csv('/content/herremans_hit_1030test.csv')\n",
        "labels = test.iloc[:,-1]\n",
        "test = test.drop('Topclass1030', axis=1)\n",
        "testdata = torch.Tensor(test.values)\n",
        "testlabels = torch.Tensor(labels.values).view(-1,1)\n",
        "\n",
        "TP = 0\n",
        "TN = 0\n",
        "FN = 0\n",
        "FP = 0\n",
        "\n",
        "for i in range(0, testdata.size()[0]): \n",
        "  # print(testdata[i].size())\n",
        "  Xtest = torch.Tensor(testdata[i])\n",
        "  y_hat = model(Xtest)\n",
        "  \n",
        "  if y_hat > 0.5:\n",
        "    prediction = 1\n",
        "  else: \n",
        "    prediction = 0\n",
        "\n",
        "  if (prediction == testlabels[i]):\n",
        "    if (prediction == 1):\n",
        "      TP += 1\n",
        "    else: \n",
        "      TN += 1\n",
        "\n",
        "  else:\n",
        "    if (prediction == 1):\n",
        "      FP += 1\n",
        "    else: \n",
        "      FN += 1\n",
        "\n",
        "print(\"True Positives: {0}, True Negatives: {1}\".format(TP, TN))\n",
        "print(\"False Positives: {0}, False Negatives: {1}\".format(FP, FN))\n",
        "rate = TP/(FN+TP)\n",
        "print(\"Class specific accuracy of correctly predicting a hit song is {0}\".format(rate))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Week 5 - AI homework - torch intro",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
